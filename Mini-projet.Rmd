---
title: "Mini  projet Stat non param"
author: "OUOROU_HOUSSEIN"
date: "2022-11-04"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Exercice 1

## 1- Fonction `densitygauss(X, h, Grid, plot = FALSE)`

Implémentation du noyau gaussien

```{r}
densitygauss = function(X, h, Grid, plot = FALSE){
  n = length(X)
  K = length(Grid)
  gaussker = matrix(0, nrow = n, ncol = K)
  for (i in 1:K) {
    for(j in 1:n){
    U = (X[j] - Grid[i] )/h
    gaussker[j, i] = (1/(h*sqrt(2*pi))) * exp(-0.5*(U^2)) 
    
    }
    
  }
  fh = apply(gaussker, 2, mean)
    if(plot == TRUE){
      dens = plot(Grid, fh, type = "l", col = 2, lwd = 1.5)
      print(dens)
    }
  return(list(x = Grid, y = fh))
}


```

## 2- Simulation 

On simule des données suivant un mélange de deux lois bêta.
```{r}
set.seed(2345)

n = 500
p = 0.5
b1 = 6; a1 = 2
b2 = 2; a2 = 6

simulmixt = function(n, aa1 = a1, bb1 = b1, aa2 = a2, bb2 = b2, pmix = p){
  
  U = runif(n)
  V = (U <= pmix)*1
  
  X = V*rbeta(n, a1, b1) + (1-V)*rbeta(n, a2, b2)
  return(X)
}
X = simulmixt(n = 500)
```

On trace l'histogramme des données simulées en rajoutant l'estimation de la densité par le noyau gaussien déjà implémenté dans R.
```{r}
hist(X, probability = TRUE, breaks = 15)
lines(density(X, kernel = "gaussian"))
```

## 3- On compare l'estimation faites par R à la notre.


```{r}
dens = densitygauss(X, h = 0.1, Grid = seq(0, 1, length.out = 40), plot = F)

hist(X, probability = TRUE, breaks = 15)
lines(density(X, kernel = "gaussian"), lwd = 2)
lines(x = dens$x, y = dens$y, col = 2, lwd = 2)
legend(x = 0.3, y = 1.5, col = c(1, 2), legend = c("R density", "densitygauss"), lty = 1, bty = "n", lwd = 2)
```

On remarque une différence entre les deux estimations de la densité réalisé par R et par notre fonction bien que les deux soient basés sur le noyau gaussien. En observant finement, on remarque que dans les endroits à faible densité, notre estimation est plus grande, mais plus petite que celle de R dans les zones à fortes densité. Cette différence est dû à la différence entre le choix de la fenêtre h.


## Exercice 2  Validation croisée

1 - Démonstration

Montrons que $(x-u_1)^2 + (x-u_2)^2 = 2(x-(\frac{u_1 + u_2}{2}))^2 + \frac{1}{2} (u_1-u_2)^2$

On partira de la deuxième égalité pour retrouver la première


$${2(x-(\frac{u_1 + u_2}{2}))^2 + \frac{1}{2} (u_1-u_2)^2  \\=
2[ x^2 - \frac{2x(u_1+u_2)}{2} + \frac{(u_1+u_2)^2}{4}] + \frac{1}{2} (u_1^2 - 2u_1u_2+u_2^2) \\=
2x^2 -2x(u_1+u_2)+\frac{1}{2}(u_1^2+2u_1u_2+u_2^2)+\frac{1}{2}(u_1^2-2u_1u_2+u_2^2) \\=
2x^2 -2x(u_1+u_2)+ u_1^2 + u_2^2 \\=
x^2 -2xu_1+u_1^2 +x^2-2xu_2+u_2^2 \\
= (x-u_1)^2+(x-u_2)^2}$$

2- 
$${\text{Mq} ~ A(h) = \int_R \hat{f_h}^2(x)~dx = \frac{1}{2n^2h \sqrt\pi} \times \sum_{1 \leq i,j \leq n} exp(- \frac{(X_i - X_j)^2}{4h^2}) \\
A(h)= \int_R ~ [\frac{1}{nh \sqrt{2\pi}} \sum_{i=1}^{n} exp (- \frac{1}{2}(\frac{X_i - x}{h})^2)]^2~ dx \\
A(h) = \frac{1}{(nh)^2 \times 2 \pi} \times \int_R \sum_{i=1}^{n} exp (- \frac{1}{2}(\frac{X_i - x}{h})^2)~ dx \\
A(h) = \frac{1}{(nh)^2 \times 2 \pi} \times \int_R \sum_{i=1}^{n} exp (- \frac{1}{2}(\frac{X_i - x}{h})^2) \times \sum_{j=1}^{n} exp (- \frac{1}{2}(\frac{X_j - x}{h})^2)~ dx \\
A(h) = \frac{1}{(nh)^2 \times 2 \pi} \times \sum_{i,j=1}^{n} \int_R exp (- \frac{1}{2h^2} \times [(x-X_i)^2+(x-X_j)^2])~dx  \\
A(h) = \frac{1}{(nh)^2 \times 2 \pi} \times \sum_{i,j=1}^{n} \int_R  exp (- \frac{1}{2h^2} \times [2(x- \frac{X_i + X_j}{2})^2+ \frac{1}{2}(X_i-X_j)^2])~dx ~~ d'après(1) \\
A(h) = \frac{1}{(nh)^2 \times 2 \pi} \times \sum_{i,j=1}^{n} \int_R  exp (- \frac{1}{4h^2}(X_i-X_j)^2) \times exp(- \frac{1}{h^2} \times (x-\frac{X_i + X_j}{2})^2) ~ dx \\
A(h) = \frac{1}{(nh)^2 \times 2 \pi} \times \sum_{i,j=1}^{n}  exp (- \frac{1}{4h^2}(X_i-X_j)^2) \times \int_R exp(- \frac{1}{h^2} \times (x-\frac{X_i + X_j}{2})^2) ~ dx \\
avec \int_R exp(- \frac{1}{h^2} \times (x-\frac{X_i + X_j}{2})^2) ~ dx = \sqrt\pi h \\
A(h)= \frac{1}{(nh)^2 \times 2 \pi} \times \sqrt\pi h \times \sum_{i,j=1}^{n}  exp (- \frac{1}{4h^2}(X_i-X_j)^2) \\
A(h) = \frac{1}{2(n)^2 h \sqrt\pi} \times  \sum_{i,j=1}^{n}  exp (- \frac{1}{4h^2}(X_i-X_j)^2)}$$

## 3- Calcul de *A(h)* 

```{r}
set.seed(1)
p=0.5; b1= a2=6; a1=b2=2; hG=0.05; n=500 ; x_max=1# U tiré dans [0,1] => on pose x_max=1
U=runif(n)
Grid=seq(0, x_max, length.out=40)# de taille 40 par choix arbitraire
V=(U<=p)# V_i vaut 1 ac proba p
X=ifelse(U<=p,rbeta(n, shape1 = a1, shape2 = b1), rbeta(n,shape1 = a2, shape2 = b2))# => X€[0,1] par déf de suivre 1 bêta

evalA = function(X, h){
  n = length(X)
  c = 1/(2*(n^2)*h*sqrt(pi))
  mat = matrix(0, nrow = n, ncol = n)
  for (i in 1:n) {
   mat[i, ] = c * sapply(X, function(xj) exp( -((X[i] - xj)^2)/(4*h^2)))
  }
  return(sum(mat))
}

# test
evalA(X, hG)
```

## 4- Calcul de *B(h)*

```{r}
evalB = function(X, h){
  n = length(X)
  c = 1/(n*(n-1)*h*sqrt(2*pi))
  mat = matrix(0, nrow = n, ncol = n)
  for (i in 1:n) {
   mat[i, ] = c * sapply(X, function(xj) exp( -((X[i] - xj)^2)/(2*h^2)))
  }
  diag(mat) = 0
  return(sum(mat))
}

evalB(X, hG)
```

## 5- Calcul de $\hat\phi(h)$

```{r}
criterecv = function(X, h){
  return(evalA(X, h) - 2*evalB(X, h))
}
criterecv(X, hG)
```

## 6- Choix de h en deux étapes

- **Etape exploratoire**
```{r}
H1 = seq(0, 1, length.out = 10)

critere = sapply(H1, function(h) criterecv(X, h))
data.frame(H1,critere)
```

Remarquons que la valeur du critère ne fait qu'augmenter a partir de $h = 0.11$. Dans ces conditions il est inutile d'explorer les valeur au delà. Vu que les valeurs sont négatives ce graphique permet de mieux visualiser.
```{r}
plot(H1, critere, pch = 19)
text(H1[2] , critere[2]+ 0.05, labels = paste("h = ", round(H1[2],2)))
```

On raffine le pas maintenant en choisissant des valeurs en 0 et 0.1
```{r}
H2 = seq(0, 0.1, length.out = 15)

critere = sapply(H2, function(h) criterecv(X, h))
hopt = H2[which.min(critere)]
plot(H2,critere, pch = 19)
points(hopt, min(critere, na.rm = T), pch = 19, col = 2)
text(hopt, min(critere, na.rm = T) + 0.003, labels = paste("Hopt"), col = 2)
```

Le h correspondant au critère minimal tourne au tour de 0.04. Plus précisément ```r hopt``` .

On essaie de comparer cette valeur optimale obtenue à celle obtenue par la fonction implémenter dans R

```{r}
estimR = density(X, kernel = "gaussian")
estimR$bw
```

la fonction  `density` donne un hopt de 0.07.

On superpose les densités estimée avec les h optimaux

```{r}
dens = densitygauss(X, h = hopt, Grid = seq(0, 1, length.out = 40), plot = F)

hist(X, probability = TRUE, breaks = 15)
lines(estimR, col = 1, lwd = 2)
lines(x = dens$x, y = dens$y, col = 2, lwd = 2)
legend(x = 0.3, y = 1.5, col = c(1, 2), legend = c("hopt by R", "hopt by us"), lty = 1, bty = "n", lwd = 2)

```

Même si les deux courbes semblent s'adapter aux données, la courbe rouge (celle obtenue avec notre h) s'adapte un peu plus mieux.

## 7- On repète la question précédente avec les données `CO2`

```{r}
data("CO2")
uptake = CO2$uptake
hist(uptake)
```

On commence par choisir des valeurs de h entre 0 et 1
```{r}
H = seq(0, 1, length.out = 10)
critere = sapply(H, function(h) criterecv(uptake, h))
data.frame(H, critere)
```
Ici le critère ne fais que diminuer jusqu'a $h = 1$ (la dernière valeur). On ne peut pas s'arreter car il se pourrait qu'il ait des valeur au dessus de 1 avec des critères plus petit. On choisit maintant des h entre 1 et 2.

```{r}
H = seq(1, 2, length.out = 10)
critere = sapply(H, function(h) criterecv(uptake, h))
data.frame(H, critere)
```
Pareil le critère ne fait que décroitre. On continue entre 2 et 4 cette fois
```{r}
H = seq(2, 4, length.out = 20)
critere = sapply(H, function(h) criterecv(uptake, h))
plot(H, critere)
data.frame(H, critere)
```
On observe un minimum autour de 3.5. 
```{r}
hopt = H[which.min(critere)]
hopt
```
On obtient un h optimal qui vaut 3.4736.

Avec R :

```{r}
estimR = density(uptake, kernel = "gaussian")
estimR$bw
```
la fonction density donne une valeur de h optimale à 4.01. On superpose les deux densités.

```{r}
dens = densitygauss(uptake, h = hopt, Grid = seq(0, 50, length.out = 40), plot = F)

hist(uptake, probability = TRUE)
lines(estimR, col = 1, lwd = 2)
lines(x = dens$x, y = dens$y, col = 2, lwd = 2)
```

# Exercie 3

```{r}
n = 500
p = 0.5
b1 = 6; a1 = 2
b2 = 2; a2 = 6
simulmixt = function(n, aa1 = a1, bb1 = b1, aa2 = a2, bb2 = b2, pmix = p){
  
  U = runif(n)
  V = (U <= pmix)*1
  
  X = V*rbeta(n, aa1, bb1) + (1-V)*rbeta(n, aa2, bb2)
  return(X)
}
```

```{r}
EstimBiasVar <- function(Nexp = 1000 , x = 0.5, h = 0.01, aa1 = a1, bb1 = b1, aa2 = a2, bb2 = b2, pmix = p ){
  EstimGauss <- rep(0,Nexp) ## vecteur pour stocker les estimateurs de la densit
  for(ind in 1:Nexp){
    X = simulmixt(n = 500, aa1, bb1, aa2, bb2 , pmix )
    KerGauss= densitygauss(X, h = h, Grid = x, plot = F)
    EstimGauss[ind] <- mean(KerGauss$y)
    #EstimGauss2[ind] <- mean(KerGauss$y^2)
    
  }
  trueDens <- dbeta(x, aa1, bb1) ## completer: vraie valeur de la densite au point xpmix
  biasGauss <- mean(EstimGauss - trueDens) ## estimateur Monte-Carlo du biais grace a EstimGauss
  varGauss <-  mean((EstimGauss - trueDens)^2)## estimateur Monte-Carlo de la variance grace a EstimGauss
  riskGauss <-varGauss + biasGauss^2## estimateur Monte-Carlo du risque quadratique
  return(list(true = trueDens, biasGauss = biasGauss, varGauss = varGauss, riskGauss = riskGauss))
}
EstimBiasVar(Nexp = 100, x = 0.65)
EstimBiasVar = Vectorize(EstimBiasVar, vectorize.args = "x")

```


```{r}
plotBiasVar <- function(hh = seq(0.01,0.15, length.out = 10),Nexp = 500, x = 0.5, aa1 = a1, bb1 = b1, aa2 = a2, bb2 = b2, pmix = p, n = 500){
  nH <- length( hh )
  biasGauss <- rep(0,nH)
  varGauss <- rep(0,nH)
  for (i in 1:nH){
    res <- EstimBiasVar(Nexp = Nexp, x=x, h = hh[i], aa1=a1,bb1=b1,aa2=a2,bb2=b2, pmix=p )
    biasGauss[i] <- res$biasGauss
    varGauss[i]<- res$varGauss
  }
  Ylim <- range(biasGauss^2, varGauss, biasRect^2, varRect)
  plot(hh, biasGauss^2, col = 'red', type = 'l', ylim = Ylim)
  lines(hh, varGauss, col = 'orange')
  lines(hh, biasGauss^2 + varGauss, col ='orange', lwd = 2, lty =3)
  legend('topleft', legend = c('Gauss sqr bias', 'Gauss var', 'Gauss Risk'),
  col = c('red', 'orange', 'orange'),
  lty = c(1,1,3))
  return(list(biasGauss = biasGauss,
  varGauss = varGauss))
}
## test

H = runif(10, 0, 5)
X = simulmixt(500)
critere = sapply(H, function(h) criterecv(X, h))
hopt = H[which.min(critere)]
HH <- seq(0.01,0.15, length.out = 10)
plotres <- plotBiasVar(hh = HH)
plotres$biasGauss^2 + plotres$varGauss -> r
hoptim = HH[which.min(r)]
segments(x0 = hoptim, y0 = 0, x1 = hoptim, y1 = min(r))
segments(x0 = 0, y0 = min(r), x1 = hoptim, y1 = min(r))
```
```{r}

plotres <- plotBiasVar(HH)

```
```{r}
densitygauss(X, h = hoptim, Grid = 0.5)

densitygauss(X, h = 0.0001, Grid = 0.5)
```
```{r}
h
xx= seq(0,1,length.out=50)
yy = xx
for(i in 1:50){
res = EstimBiasVar(Nexp = 1, x = xx[i], h=h)
yy[i] = res$biasGauss + res$true
}
plot(xx,yy, type = "l")
```
```{r}
# xx
# r = c()
# for (i in 1:length(HH)) {
#  r[i] =  mean(EstimBiasVar(Nexp = 200, x = xx, h = HH[i])[4, ] |> unlist())
# }
# 
# plot(HH, r, type = "l")



```


