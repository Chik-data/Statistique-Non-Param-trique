---
title: "Projet_SNP"
author: "Chikhaoui Sarah"
date: "2022-10-30"
output: html_document
---

```{r setup, include=FALSE}
set.seed(1)
```

```{r}
densityGauss<-function(X, h, Grid, choise.plot=TRUE){
  
  L <-length(Grid)
  K=matrix(NA, nrow = n, ncol = L)
  for (i in 1:L){
    
    K[ ,i]= (1/sqrt(2*pi))* exp(-((X-Grid[i])^2)/ (2*h^2))     
    
  }
  y=apply(X =K,MARGIN = 2,FUN = mean)
  Y= y/h
  
  if(choise.plot == TRUE){
      plot(Grid, Y, type = "l", col = 2, lwd = 1.5)#, col='purple', main="Estimateur à Noyau Gaussien d'1 echantillon X", xlab="echantillon X", ylab="f_chap"
    }
  
  return(list(x = Grid, y = Y))
}

```


```{r}
#L’estimateur dépend de 2 paramètres: le point d’origine t0 et la largeur de classe h="bin width".
set.seed(1)
p=0.5; b1= a2=6; a1=b2=2; hG=0.05; n=500 ; x_max=1# U tiré dans [0,1] => on pose x_max=1
U=runif(n)
Grid=seq(0, x_max, length.out=40)# de taille 40 par choix arbitraire
V=(U<=p)# V_i vaut 1 ac proba p
X=ifelse(U<=p,rbeta(n, shape1 = a1, shape2 = b1), rbeta(n,shape1 = a2, shape2 = b2))# => X€[0,1] par déf de suivre 1 bêta

#Test visuel du rés. attendu par ma fonction
densityGauss(X, hG, Grid, choise.plot = TRUE)
lines(density(X, kernel = "gaussian"))
```

#Exo2
1 . (question préliminaire) Vérifiez que pour tous réels $x~~\mu_{1}~~\mu_{2}$, on a
$$(x-\mu_{1})^{2}+(x-\mu_{2})^{2} = 2 (x-(\mu_{1}+\mu_{2})/2)^{2}+\frac{1}{2}(\mu_{1}+\mu_{2})^{2}$$

\begin{eqnarray*}
     2 (x-(\mu_{1}+\mu_{2})/2)^{2}+\frac{1}{2}(\mu_{1}+\mu_{2})^{2} &=& 2 (x^{2}-x(\mu_{1}+\mu_{2})+((\mu_{1}+\mu_{2})/2)^{2}
     + \frac{1}{2}(\mu_{1}^{2}+2\mu_{1}\mu_{2}+\mu_{2}^{2})\\
     &=&  2 (x^{2}-x(\mu_{1}+\mu_{2})+(\mu_{1}^{2}+2\mu_{1}\mu_{2}+\mu_{2}^{2})/4
     + \frac{1}{2}(\mu_{1}^{2}+2\mu_{1}\mu_{2}+\mu_{2}^{2})\\
     &=&  2 x^{2}-2x(\mu_{1}+\mu_{2})+(\mu_{1}^{2}+2\mu_{1}\mu_{2}+\mu_{2}^{2})/2
     + \frac{1}{2}(\mu_{1}^{2}+2\mu_{1}\mu_{2}+\mu_{2}^{2})\\
     &=&  2 x^{2}-2x(\mu_{1}+\mu_{2})+ \mu_{1}^{2}+ \mu_{2}^{2}\\
     &=&   x^{2}-2x\mu_{1} + \mu_{1}^{2}+ x^{2}-2x\mu_{2}+ \mu_{2}^{2}\\
     &=& (x-\mu_{1})^{2}+(x-\mu_{2})^{2} 
\end{eqnarray*}

2. En développant l’expression $\hat f_{h}(x) ^{2}$
à partir de l’expression (1), en exploitant l’identité (2) et en
utilisant le fait que pour tout $\mu \in \mathbf{R}$ fixé, $\int 
K_{h}(x - \mu) dx = 1$ , montrer que

\begin{equation*}
    \int_{\mathbf{R}} \hat f_{h}(x) ^{2}d_{x} =  \frac{1}{2n^{2}h \sqrt{\pi }}    \sum_{1\leq i,j\leq n}^{ }  e^{-\frac{(X_{i}+X_{j})^{2}}{4h^{2}}}
\end{equation*}

\begin{eqnarray*}
     \int_{\mathbf{R}} \bigg[\hat f_{h}(x) \bigg]^{2}d_{x}&=& \int_{\mathbf{R}} \Bigg[\frac{1}{nh} \sum_{i=1}^{n} K \bigg(\frac{x-X_{i}}{h}\bigg) \Bigg]^{2}dx\\
     &=& \bigg(\frac{1}{nh}\bigg)^{2} \int_{\mathbf{R}} \Bigg[ \sum_{i=1}^{n} K \bigg(\frac{x-X_{i}}{h}\bigg) \Bigg]^{2}dx\\
     &=& \bigg(\frac{1}{nh}\bigg)^{2} \int_{\mathbf{R}} \Bigg[ \sum_{i=1}^{n} \frac{1}{\sqrt{2\pi}} \exp \bigg(-\frac{1}{2} \bigg(\frac{x-X_{i}}{h}\bigg)^{2}\bigg) \Bigg]^{2}dx\\
     &=& \bigg(\frac{1}{nh}\bigg)^{2} \frac{1}{2\pi} \int_{\mathbf{R}} \Bigg[ \sum_{i=1}^{n}  \exp \bigg(-\frac{1}{2} \bigg(\frac{x-X_{i}}{h}\bigg)^{2}\bigg) \Bigg]^{2}dx
\end{eqnarray*}

On sait que  $ \bigg(\sum_{i}^{~ }  X_{i}\bigg)^{2} = \sum_{i,j} ^{~~ } (X_{i})^{2}(X_{j})^{2}$

\begin{eqnarray*}
     \int_{\mathbf{R}} \bigg[\hat f_{h}(x) \bigg]^{2}d_{x}&=&\bigg(\frac{1}{nh}\bigg)^{2} \frac{1}{2\pi} \int_{\mathbf{R}}  \sum_{1\leq i,j\leq n}^{ }  \exp \bigg(-\frac{1}{2} \bigg(\frac{x-X_{i}}{h}\bigg)^{2}\bigg)  \exp \bigg(-\frac{1}{2} \bigg(\frac{x-X_{j}}{h}\bigg)^{2}\bigg) dx\\
     &=& \bigg(\frac{1}{nh}\bigg)^{2} \frac{1}{2\pi} \int_{\mathbf{R}}  \sum_{1\leq i,j\leq n}^{ }  \exp \Bigg(-\frac{1}{2h^{2}} \bigg[ (x-X_{i})^{2} + (x-X_{j})^{2}\bigg]\Bigg) dx\\
\end{eqnarray*}
 
 D'après la question précédente, on a  $(x-\mu_{1})^{2}+(x-\mu_{2})^{2} = 2 (x-(\mu_{1}+\mu_{2})/2)^{2}+\frac{1}{2}(\mu_{1}+\mu_{2})^{2}$,
 
 \begin{eqnarray*}
     \int_{\mathbf{R}} \bigg[\hat f_{h}(x) \bigg]^{2}d_{x}&=&
      \bigg(\frac{1}{nh}\bigg)^{2} \frac{1}{2\pi} \int_{\mathbf{R}}  \sum_{1\leq i,j\leq n}^{ }  \exp \Bigg(-\frac{1}{2h^{2}} \bigg[  2 (x-(X_{i}+X_{j})/2)^{2}+\frac{1}{2}(X_{i}+X_{j})^{2}\bigg]\Bigg) dx\\
      &=& \bigg(\frac{1}{nh}\bigg)^{2} \frac{1}{2\pi} \sum_{1\leq i,j\leq n}^{ } \int_{\mathbf{R}}    \exp \Bigg(-\frac{1}{4h^{2}}(X_{i}+X_{j})^{2}\Bigg) \exp\Bigg( -\frac{1}{h^{2}} \bigg[   (x-(X_{i}+X_{j})/2)^{2}\bigg]\Bigg) dx\\
      &=& \bigg(\frac{1}{nh}\bigg)^{2} \frac{1}{2\pi}    \sum_{1\leq i,j\leq n}^{ }  \exp \Bigg(-\frac{1}{4h^{2}}(X_{i}+X_{j})^{2}\Bigg) \int_{\mathbf{R}}  \exp\Bigg( -\frac{1}{h^{2}} \bigg[   (x-(X_{i}+X_{j})/2)^{2}\bigg]\Bigg) dx\\
    \end{eqnarray*}
Puisque, $\forall ~~\alpha~,~~ \int_{\mathbf{R}} e^{- \alpha x^{2}} = \sqrt{\frac{\pi}{\alpha}}$, on a 
    
  \begin{eqnarray*}
     \int_{\mathbf{R}} \bigg[\hat f_{h}(x) \bigg]^{2}d_{x}&=&
     \bigg(\frac{1}{nh}\bigg)^{2} \frac{1}{2\pi}    \sum_{1\leq i,j\leq n}^{ }  \exp \Bigg(-\frac{1}{4h^{2}}(X_{i}+X_{j})^{2}\Bigg)  \sqrt{\frac{\pi}{\frac{1}{h^{2}}}}  \\
     &=& \bigg(\frac{1}{nh}\bigg)^{2} \frac{1}{2\pi}   \sqrt{\pi h^{2}} \sum_{1\leq i,j\leq n}^{ }  \exp \Bigg(-\frac{1}{4h^{2}}(X_{i}+X_{j})^{2}\Bigg) 
    \end{eqnarray*}
Après simplification, on obtient 
\begin{equation*}
    \int_{\mathbf{R}} \hat f_{h}(x) ^{2}d_{x} =  \frac{1}{2n^{2}h \sqrt{\pi }}    \sum_{1\leq i,j\leq n}^{ }  e^{-\frac{(X_{i}+X_{j})^{2}}{4h^{2}}}
\end{equation*}


3. Ecrire une fonction evalA(X,h) prenant en argument un vecteur de données X de taille $n$ et une
fenêtre $h$, et renvoyant la valeur de $A(h)$.
```{r}
evalA=function(X=vector(mode = "numeric"), h){
  n= length(X)
  aah=matrix(NA, nrow =n , ncol=n)
  facteur=1/(2*h*sqrt(pi)*(n^2))
    for (j in 1:n) {
      aah[j, ]=facteur*sapply(X, function(Xi) exp(-  ((Xi-X[j])/(2*h))^2 ) )
    }
   # aah=sapply(aah, 1,mean)#
   # Ah=facteur*apply(aah, 2, mean)
  return(sum(aah))
}

#Test de ma fonction A(h):
evalA(X, hG)
```

```{r}
evalB=function(X=vector(mode = "numeric", length = n), h){
  n=length(X)
  bbh=matrix(NA, nrow =n , ncol=n)
  facteur=1/( h*sqrt(2*pi)*n*(n-1) )
    for (j in 1:n) { 
      bbh[j, ]=facteur*sapply(X, function(Xi) exp(-( ((Xi-X[j])  /h )^2)/2 ) )
   }

  #bbh=facteur*apply(bbh, 1, exp)
  diag(bbh)<-0
  #Bh=mean(apply(bbh, 2, sum)/(n-1))
  return(sum(bbh))
}
#Test de ma fonction B(h):
evalB(X, hG)
```


```{r}
critereCV=function(X=vector(mode = "numeric", length = n), h){
  Phi_h=evalA(X,h) -2*evalB(X, h)
  return(Phi_h)
}
#Test de ma fonction:
critereCV(X, hG)
```

6- En utilisant une grille $\mathcal{H}$ bien choisie (en deux étapes, une étape exploratoire et une étape pour raffiner le pas), donner une estimation hopt de la fenêtre optimale pour le jeu de données simulées.

1.Etape exploratoire:
````{r}
H=seq(0, 1, length.out = 20)
critere = sapply(H, function(h) critereCV(X, h))
(hopt = H[which.min(critere)])
tmp=cbind.data.frame(H,critere)[-1,]
```

On observe que la valeur du critère ne fait qu'augmenter a partir de $h = 0.052$. Visualisons les valeurs la valeur du critère en fonction de notre grille H:
```{r}
plot(tmp$H, tmp$critere, pch = 16, xlab ="h", ylab = "critere", main = "critere en fonction de valeur de h")
```

2. Etape de raffinage du pas: en choisissant des valeurs entre 0 et 0.1:
```{r}
H1 = seq(0, 0.1, length.out = 25)
critere = sapply(H1, function(h) critereCV(X, h))
(hopt = H1[which.min(critere)])
plot(H1, critere, pch = 16, xlab ="h", ylab = "critere", main = "critere en fonction de valeur de h")
points(hopt, min(critere, na.rm = T), pch = 19, col = 2)
text(hopt, min(critere, na.rm = T) + 0.003, labels = paste("hopt"), col = 2)
```

Ainsi le h qui vérifie le critère minimal est compris entre de 0.04 et 0.05. 
Comparons la valeur de h optimale trouvé par nôtre fonction à celle donnée par la fonction R:
```{r}
estim.density= density(X, kernel = "gaussian")
(hopt.by.r = density(X, kernel = "gaussian")$bw)
```

la fonction  `density` donne un hopt de 0.075.

On superpose les densités estimées avec les h optimaux otenues par chaque fonction:

```{r}
Grid = seq(0, 1, length.out = 40)
dens.by.us = densityGauss(X, h = hopt, Grid=Grid , choise.plot= F)

hist(X, probability = TRUE, breaks = 15)
lines(estim.density, col = 'purple', lwd = 2)
lines(x = dens.by.us$x, y = dens.by.us$y, col = 2, lwd = 2)
legend(x = 0.3, y = 1.5, col = c('purple', 2), legend = c("hopt by R", "hopt by us"), lty = 1, bty = "n", lwd = 2)

```

Il semblerait que les 2 courbes s'ajustent bien aux données néanmoins, notre courbe (rouge) semble mieux s'ajuster.

## 7- On repète la question précédente avec les données `CO2`
```{r}
data("CO2")
hist(CO2$uptake)
```

```{r}
H = seq(0, 0.1, length.out = 25)
critere = sapply(H, function(h) critereCV(CO2$uptake, h))
data.frame(H, critere)
```

```{r}
(hopt = H[which.min(critere)])
estimateur = density(CO2$uptake, kernel = "gaussian")
estimateur$bw
```


```{r}
#dens = densityGauss(CO2$uptake, h = hopt, Grid = seq(0, x_max, length.out=40), choise.plot = F)
hist(CO2$uptake, probability = TRUE)
 lines(estimateur, col = 1, lwd = 2)
#lines(x = dens$x, y = dens$y, col = 2, lwd = 2)
```



Part.3:
Exercice 3

```{r}
p=0.5;a1=2;b1=6;a2=6;b2=2
n=500
simMixture<-function(n,aa1=a1,bb1=b1,aa2=a2,bb2=b2,pmix=p)
{
  U<-runif(n,0,1)
  U
  V=(U<=pmix)
  V
  X=V*rbeta(n,aa1,bb1)+(1-V)*rbeta(n,aa2,bb2)
  X
  return(X)
}

simMixture(5)
hist(simMixture(500), xlim = c(0,1), ylim=c(0,80))
```


```{r}
Nexp=1000;pmix=p=0.5;x=0.5;aa1=a1;bb1=b1;aa2=a2;bb2=b2;pmix=p;h=0.01
EstimBiasVar<-function(Nexp=1000,n=500,x=0.5,h=0.01,aa1=a1,bb1=b1,aa2=a2,bb2=b2,pmix=p){
  EstimGauss<-rep(0,Nexp)
  EstimGauss
  for (ind in 1:Nexp)
  {
    X=simMixture(n,aa1,bb1,aa2,bb2,pmix)
    
    
    KerGauss=exp(-0.5*((X-x)/h)^2)/(sqrt(2*pi)*h)
    
    EstimGauss[ind]<-mean(KerGauss)
  }
  
  trueDens<-pmix*dbeta(x,2,6)+(1-pmix)*dbeta(x,6,2)
  
  biasGauss<-mean(EstimGauss)-trueDens
  varGauss<-var(EstimGauss)
  riskGauss<-biasGauss^2+varGauss
  
  
  return(list(true=trueDens, biasGauss=biasGauss, varGauss=varGauss,
              riskGauss=riskGauss))
}

```


```{r}
####B.1 test:

h<-0.05
res<-EstimBiasVar(h=h)
res$riskGauss

xx=seq(0,1,length.out=100)
yy=xx
for (i in 1:100){
  res= EstimBiasVar(Nexp=10, x=xx[i], h=h)
  yy[i]=res$biasGauss+res$true
}

f_vraie=p*dbeta(xx,a1,b1)+(1-p)*dbeta(xx,a2,b2)
plot(xx,yy, pch=20, col='red', ylim=c(0,1.7))
lines(xx,f_vraie, lwd=4 )


```



```{r}
plotBiasVar <- function(hh = seq(0.01,0.15, length.out = 10),
Nexp = 500, x = 0.5, aa1 = a1, bb1 = b1,
aa2 = a2, bb2 = b2, pmix = p, n = 2000,plot=F){
  nH <- length( hh )
  biasGauss <- rep(0,nH)
  varGauss <- rep(0,nH)
  for (i in 1: nH){
    res <- EstimBiasVar(Nexp = Nexp, x=x, h = hh[i])
    biasGauss[i] <- res$biasGauss
    varGauss[i]<- res$varGauss
  }
  if(plot){
  Ylim <- range(biasGauss^2, varGauss)
  plot(hh, biasGauss^2, col = 'red', type = 'l', ylim = Ylim, lwd=4)
  lines(hh, varGauss, col = 'black', lwd=4)
  lines(hh, biasGauss^2 + varGauss, col ='blue', lwd = 4, lty =3)
  legend('topleft', legend = c('Gauss sqr bias', 'Gauss var', 'Gauss Risk' ),
  col = c('red', 'black', 'blue'),lty = c(1,1,3))
  }
  return(list(biasGauss = biasGauss,
  varGauss = varGauss))
}


```

```{r}
###test
HH<-seq(0.01,0.15, length.out=10)
plotres<-plotBiasVar(HH,plot=TRUE)
plotres

```

```{r}
###trouver le h minimisant le risque quadratique?
iGauss<-which.min(plotres$biasGauss^2+plotres$varGauss) #indice dans la grille HH pour lequel risque est minimal
HH[iGauss]
```

Discussion

Après implementation, le h obtenue est $h^{*} = 0.05666667$ et d'après l'exercice précédente par validation coisée, est hopt obtenu est hopt = 0.04583333 et ces deux valeur sont senciblement egale.


